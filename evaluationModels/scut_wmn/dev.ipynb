{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FCSRN import fcsrn\n",
    "#from FCSRN_AugLoss import fcsrn\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 48; WIDTH = 160; CHANNELS = 3\n",
    "IMAGE_SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "N_TRAIN = 4000\n",
    "N_TEST = 1000\n",
    "\n",
    "BATCHES_TRAIN = N_TRAIN / BATCH_SIZE\n",
    "BATCHES_TEST = N_TEST / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize image and add padding on one dimensions to keep ratios\n",
    "def resize_withPadding(image, targetWidth=WIDTH, targetHeight=HEIGHT):    \n",
    "    inputWidth = image.shape[1]\n",
    "    inputHeight = image.shape[0]\n",
    "    # scale either width or height, depending on which scaling factor would be smaller\n",
    "    scale_width = targetWidth / inputWidth\n",
    "    scale_height = targetHeight / inputHeight\n",
    "    \n",
    "    if scale_width < scale_height:\n",
    "        # scale width, pad height\n",
    "        result = cv2.resize(image, dsize=(0,0), fx=scale_width, fy=scale_width)\n",
    "        padding = targetHeight - result.shape[0]\n",
    "        p_top = int(padding/2)\n",
    "        p_bot = p_top if (padding%2) == 0 else p_top + 1\n",
    "        assert padding >= 0 and (p_top + p_bot) == padding, \"unexpected height-padding: %d\"%(padding)\n",
    "        result = cv2.copyMakeBorder(result, top=p_top, bottom=p_bot, left=0, right=0, borderType=cv2.BORDER_CONSTANT,value=0)\n",
    "    else:\n",
    "        # scale height, pad width\n",
    "        result = cv2.resize(image, dsize=(0,0), fx=scale_height, fy=scale_height)\n",
    "        padding = targetWidth - result.shape[1]\n",
    "        p_left = int(padding/2)\n",
    "        p_right = p_left if (padding%2) == 0 else p_left + 1\n",
    "        assert padding >= 0 and (p_left + p_right) == padding, \"unexpected width-padding: %d\"%(padding)\n",
    "        result = cv2.copyMakeBorder(result, top=0, bottom=0, left=p_left, right=p_right, borderType=cv2.BORDER_CONSTANT,value=0)                                    \n",
    "    return result\n",
    "\n",
    "# open and resize an image\n",
    "def loadImage(imgPath, height=HEIGHT, width=WIDTH):\n",
    "    image = cv2.imread(str(imgPath), cv2.IMREAD_COLOR)\n",
    "    image = resize_withPadding(image, width, height)\n",
    "    assert image.shape[0] == height and image.shape[1] == width, \"resizing failed\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n",
      "C:\\Users\\andre\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 48, 160, 3)\n",
      "(4000, 5)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "datasetPath = Path(\"C:/Users/andre/Desktop/m/datasets/SCUT-WMN DataSet\")\n",
    "trainfile = datasetPath / \"difficult_samples_for_train.txt\"\n",
    "testfile = datasetPath / \"difficult_samples_for_test.txt\"\n",
    "\n",
    "# load trainimages\n",
    "df_train = pd.read_csv(trainfile, sep=\"[ ,]\" ,header=None)\n",
    "images_train = np.array([\n",
    "    loadImage(datasetPath / row[0]) for row in df_train.values\n",
    "]).astype(\"float32\")\n",
    "labels_train = np.array([\n",
    "    row[1:] for row in df_train.values\n",
    "]).astype(\"int\")\n",
    "\n",
    "# load testimages\n",
    "df_test = pd.read_csv(testfile, sep=\"[ ,]\" ,header=None)\n",
    "images_test = np.array([\n",
    "    loadImage(datasetPath / row[0]) for row in df_test.values\n",
    "]).astype(\"float32\")\n",
    "labels_test = np.array([\n",
    "    row[1:] for row in df_test.values\n",
    "]).astype(\"int\")\n",
    "\n",
    "print(images_train.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(None, 48, 160, 3), dtype=tf.float32, name=None)\n",
      "TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)\n"
     ]
    }
   ],
   "source": [
    "# make tf-datasets\n",
    "train_X = tf.data.Dataset.from_tensor_slices(images_train[0:N_TRAIN])\n",
    "train_y = tf.data.Dataset.from_tensor_slices(labels_train[0:N_TRAIN])\n",
    "\n",
    "test_X = tf.data.Dataset.from_tensor_slices(images_test[0:N_TEST])\n",
    "test_y = tf.data.Dataset.from_tensor_slices(labels_test[0:N_TEST])\n",
    "\n",
    "train_X = train_X \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "    \n",
    "train_y = train_y \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "test_X = test_X \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "    \n",
    "test_y = test_y \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(train_X.element_spec)\n",
    "print(train_y.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoint:  D:\\m2\\savedmodels\\FCSRN_with_initializer\\epoch-100\n"
     ]
    }
   ],
   "source": [
    "model_savepath = Path(\"D:/m2/savedmodels/FCSRN_with_initializer\")\n",
    "model = fcsrn(IMAGE_SHAPE, checkpoint_path=model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Training %d batches of size %d for %d epochs\" % (BATCHES_TRAIN, BATCH_SIZE, EPOCHS) )\n",
    "#model.train(train_X, train_y, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:5811: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "lcr: 0.856000\n",
      "ar: 0.968600\n"
     ]
    }
   ],
   "source": [
    "total_characters = N_TEST * 5\n",
    "lcr, ar, mismatches = model.accuracy(test_X, test_y, BATCH_SIZE, total_characters, N_TEST)\n",
    "print(\"lcr: %f\" % (lcr))\n",
    "print(\"ar: %f\" % (ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'print(mismatches[0][2])\\nsum_dist = 0\\nfor row in mismatches:\\n    s = \"\"\\n    s += str(row[0].numpy())\\n    s += str(row[1].numpy())\\n    s += str(row[2].numpy())\\n    print(s)\\n    sum_dist += row[2]\\nprint(sum_dist)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"print(mismatches[0][2])\n",
    "sum_dist = 0\n",
    "for row in mismatches:\n",
    "    s = \"\"\n",
    "    s += str(row[0].numpy())\n",
    "    s += str(row[1].numpy())\n",
    "    s += str(row[2].numpy())\n",
    "    print(s)\n",
    "    sum_dist += row[2]\n",
    "print(sum_dist)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
