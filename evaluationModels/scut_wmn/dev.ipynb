{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "print(x[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from FCSRN import fcsrn\n",
    "#from FCSRN_AugLoss import fcsrn\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "#import difflib\n",
    "import edit_distance\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 48; WIDTH = 160; CHANNELS = 3\n",
    "IMAGE_SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 100\n",
    "N_TRAIN = 4000\n",
    "N_TEST = 1000\n",
    "\n",
    "BATCHES_TRAIN = N_TRAIN / BATCH_SIZE\n",
    "BATCHES_TEST = N_TEST / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize image and add padding on one dimensions to keep ratios\n",
    "def resize_withPadding(image, targetWidth=WIDTH, targetHeight=HEIGHT):    \n",
    "    inputWidth = image.shape[1]\n",
    "    inputHeight = image.shape[0]\n",
    "    # scale either width or height, depending on which scaling factor would be smaller\n",
    "    scale_width = targetWidth / inputWidth\n",
    "    scale_height = targetHeight / inputHeight\n",
    "    \n",
    "    if scale_width < scale_height:\n",
    "        # scale width, pad height\n",
    "        result = cv2.resize(image, dsize=(0,0), fx=scale_width, fy=scale_width)\n",
    "        padding = targetHeight - result.shape[0]\n",
    "        p_top = int(padding/2)\n",
    "        p_bot = p_top if (padding%2) == 0 else p_top + 1\n",
    "        assert padding >= 0 and (p_top + p_bot) == padding, \"unexpected height-padding: %d\"%(padding)\n",
    "        result = cv2.copyMakeBorder(result, top=p_top, bottom=p_bot, left=0, right=0, borderType=cv2.BORDER_CONSTANT,value=0)\n",
    "    else:\n",
    "        # scale height, pad width\n",
    "        result = cv2.resize(image, dsize=(0,0), fx=scale_height, fy=scale_height)\n",
    "        padding = targetWidth - result.shape[1]\n",
    "        p_left = int(padding/2)\n",
    "        p_right = p_left if (padding%2) == 0 else p_left + 1\n",
    "        assert padding >= 0 and (p_left + p_right) == padding, \"unexpected width-padding: %d\"%(padding)\n",
    "        result = cv2.copyMakeBorder(result, top=0, bottom=0, left=p_left, right=p_right, borderType=cv2.BORDER_CONSTANT,value=0)                                    \n",
    "    return result\n",
    "\n",
    "# open and resize an image\n",
    "def loadImage(imgPath, height=HEIGHT, width=WIDTH):\n",
    "    image = cv2.imread(str(imgPath), cv2.IMREAD_COLOR)\n",
    "    image = resize_withPadding(image, width, height)\n",
    "    assert image.shape[0] == height and image.shape[1] == width, \"resizing failed\"\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\ipykernel_launcher.py:7: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  import sys\n",
      "C:\\Users\\andre\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\ipykernel_launcher.py:16: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 48, 160, 3)\n",
      "(4000, 5)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "datasetPath = Path(\"C:/Users/andre/Desktop/m/datasets/SCUT-WMN DataSet\")\n",
    "trainfile = datasetPath / \"difficult_samples_for_train.txt\"\n",
    "testfile = datasetPath / \"difficult_samples_for_test.txt\"\n",
    "\n",
    "# load trainimages\n",
    "df_train = pd.read_csv(trainfile, sep=\"[ ,]\" ,header=None)\n",
    "images_train = np.array([\n",
    "    loadImage(datasetPath / row[0]) for row in df_train.values\n",
    "]).astype(\"float32\")\n",
    "labels_train = np.array([\n",
    "    row[1:] for row in df_train.values\n",
    "]).astype(\"int\")\n",
    "\n",
    "# load testimages\n",
    "df_test = pd.read_csv(testfile, sep=\"[ ,]\" ,header=None)\n",
    "images_test = np.array([\n",
    "    loadImage(datasetPath / row[0]) for row in df_test.values\n",
    "]).astype(\"float32\")\n",
    "labels_test = np.array([\n",
    "    row[1:] for row in df_test.values\n",
    "]).astype(\"int\")\n",
    "\n",
    "print(images_train.shape)\n",
    "print(labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorSpec(shape=(None, 48, 160, 3), dtype=tf.float32, name=None)\n",
      "TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)\n"
     ]
    }
   ],
   "source": [
    "# make tf-datasets\n",
    "train_X = tf.data.Dataset.from_tensor_slices(images_train[0:N_TRAIN])\n",
    "train_y = tf.data.Dataset.from_tensor_slices(labels_train[0:N_TRAIN])\n",
    "\n",
    "test_X = tf.data.Dataset.from_tensor_slices(images_test[0:N_TEST])\n",
    "test_y = tf.data.Dataset.from_tensor_slices(labels_test[0:N_TEST])\n",
    "\n",
    "train_X = train_X \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "    \n",
    "train_y = train_y \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "test_X = test_X \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "    \n",
    "test_y = test_y \\\n",
    "    .cache() \\\n",
    "    .batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(train_X.element_spec)\n",
    "print(train_y.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build/load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpointpath given, model will not be saved.\n"
     ]
    }
   ],
   "source": [
    "model_savepath = None#Path(\"D:/m2/savedmodels/FCSRN_with_initializer\")\n",
    "model = fcsrn(IMAGE_SHAPE, checkpoint_path=model_savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 took: 6.723076 seconds\n",
      "step 1 took: 0.722070 seconds\n",
      "step 2 took: 0.945475 seconds\n",
      "step 3 took: 0.945473 seconds\n",
      "step 4 took: 0.984369 seconds\n",
      "step 5 took: 0.965648 seconds\n",
      "step 6 took: 0.949462 seconds\n",
      "step 7 took: 0.959407 seconds\n",
      "step 8 took: 0.959435 seconds\n",
      "step 9 took: 0.953584 seconds\n",
      "step 10 took: 0.965445 seconds\n",
      "step 11 took: 0.954421 seconds\n",
      "step 12 took: 0.938490 seconds\n",
      "step 13 took: 0.938493 seconds\n",
      "step 14 took: 0.933504 seconds\n",
      "step 15 took: 0.938490 seconds\n",
      "step 16 took: 0.956912 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bbfa720952f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(\"Training %d batches of size %d for %d epochs\" % (BATCHES_TRAIN, BATCH_SIZE, EPOCHS) )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\jupyter_ws\\evaluationModels\\scut_wmn\\FCSRN.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_X, train_y, epochs, batchsize)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minputBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetBatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mstepstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargetBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"step %d took: %f seconds\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstepstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#print(\"Training %d batches of size %d for %d epochs\" % (BATCHES_TRAIN, BATCH_SIZE, EPOCHS) )\n",
    "model.train(train_X, train_y, EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andre\\Miniconda3\\envs\\keras_gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py:5811: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "lcr: 0.856000\n",
      "ar: 0.968600\n"
     ]
    }
   ],
   "source": [
    "total_characters = N_TEST * 5\n",
    "lcr, ar, mismatches = model.accuracy(test_X, test_y, BATCH_SIZE, total_characters, N_TEST)\n",
    "print(\"lcr: %f\" % (lcr))\n",
    "print(\"ar: %f\" % (ar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# takes array of predicted-truth pairs where at least one predictionerror occured.\n",
    "# outputs arrays containing which digits exactly where predicted wrong.\n",
    "####\n",
    "# mimatches: output from fcsrn.accuracy()\n",
    "####\n",
    "# returns: (\n",
    "#    error_replace-array: (predicted digit, expected digit), predicted label, truthlabel\n",
    "#    error_extra:         extra predicted digit, predicted label, truthlabel\n",
    "#    error_missing:       missing predicted digit, predicted label, truthlabel\n",
    "#)\n",
    "####\n",
    "def list_errors(mismatches):\n",
    "    error_replace = [] # wrongly predicted digits\n",
    "    error_extra = [] # errors where too many characters predicted\n",
    "    error_missing = [] # too few characters predicted\n",
    "    # iterate every sample with mispredictions\n",
    "    for pred, truth, diff in mismatches:\n",
    "        # ignore -1/BLANK digits at end of pred\n",
    "        index_blank = np.argwhere(pred==-1)\n",
    "        index_blank = index_blank[0,0] if len(index_blank) > 0 else None\n",
    "        pred = pred[0:index_blank]\n",
    "        \n",
    "        # get minimal editdistance with ops\n",
    "        matcher = edit_distance.SequenceMatcher(pred, truth)\n",
    "        for op, i1,i2, j1,j2 in matcher.get_opcodes():\n",
    "            if op == \"replace\":\n",
    "                # one or more characters were not predicted correctly\n",
    "                wrong_predictions = pred[i1:i2]\n",
    "                expected_predictions = truth[j1:j2]\n",
    "                for pair in zip(wrong_predictions,expected_predictions):\n",
    "                    error_replace.append((pair, pred, truth))   \n",
    "            elif op == \"delete\":\n",
    "                # one or more characters more than necessary predicted\n",
    "                extra_predictions = pred[i1:i2]            \n",
    "                for extra_pred in extra_predictions:\n",
    "                    assert extra_pred != -1, \"blank encountered\"                \n",
    "                    error_extra.append((extra_pred, pred, truth))\n",
    "            elif op == \"insert\":\n",
    "                # one or more characters were not predicted at all\n",
    "                missed_predictions = truth[j1:j2]\n",
    "                for missing_pred in missed_predictions:\n",
    "                    error_missing.append((missing_pred, pred, truth))\n",
    "    error_replace = np.array(error_replace)\n",
    "    error_extra = np.array(error_extra)\n",
    "    error_missing = np.array(error_missing)\n",
    "    return (error_replace, error_extra, error_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 3)\n",
      "(2, 3)\n",
      "(9, 3)\n"
     ]
    }
   ],
   "source": [
    "err_replace, err_extra, err_missing = list_errors(mismatches)\n",
    "print(err_replace.shape)\n",
    "print(err_extra.shape)\n",
    "print(err_missing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'predicted, truth'\n",
      "[   (10, 0),\n",
      "    (1, 0),\n",
      "    (10, 0),\n",
      "    (19, 0),\n",
      "    (15, 0),\n",
      "    (1, 0),\n",
      "    (19, 0),\n",
      "    (10, 0),\n",
      "    (10, 0),\n",
      "    (19, 0),\n",
      "    (6, 0),\n",
      "    (7, 1),\n",
      "    (17, 1),\n",
      "    (4, 1),\n",
      "    (11, 2),\n",
      "    (11, 2),\n",
      "    (12, 2),\n",
      "    (14, 2),\n",
      "    (12, 2),\n",
      "    (16, 2),\n",
      "    (3, 2),\n",
      "    (13, 3),\n",
      "    (13, 3),\n",
      "    (12, 3),\n",
      "    (12, 3),\n",
      "    (12, 3),\n",
      "    (12, 3),\n",
      "    (14, 4),\n",
      "    (14, 4),\n",
      "    (13, 4),\n",
      "    (14, 4),\n",
      "    (14, 4),\n",
      "    (13, 4),\n",
      "    (14, 4),\n",
      "    (13, 4),\n",
      "    (11, 4),\n",
      "    (12, 5),\n",
      "    (3, 5),\n",
      "    (15, 5),\n",
      "    (14, 5),\n",
      "    (15, 5),\n",
      "    (14, 5),\n",
      "    (15, 6),\n",
      "    (8, 6),\n",
      "    (5, 6),\n",
      "    (15, 6),\n",
      "    (15, 6),\n",
      "    (15, 6),\n",
      "    (8, 6),\n",
      "    (5, 6),\n",
      "    (8, 6),\n",
      "    (16, 6),\n",
      "    (15, 6),\n",
      "    (5, 6),\n",
      "    (17, 7),\n",
      "    (17, 7),\n",
      "    (17, 7),\n",
      "    (17, 7),\n",
      "    (17, 7),\n",
      "    (17, 7),\n",
      "    (16, 7),\n",
      "    (17, 7),\n",
      "    (17, 7),\n",
      "    (6, 8),\n",
      "    (6, 8),\n",
      "    (18, 8),\n",
      "    (18, 8),\n",
      "    (6, 8),\n",
      "    (17, 8),\n",
      "    (9, 8),\n",
      "    (6, 8),\n",
      "    (9, 8),\n",
      "    (3, 8),\n",
      "    (18, 8),\n",
      "    (18, 8),\n",
      "    (19, 9),\n",
      "    (18, 9),\n",
      "    (8, 9),\n",
      "    (19, 9),\n",
      "    (19, 9),\n",
      "    (18, 9),\n",
      "    (19, 9),\n",
      "    (19, 9),\n",
      "    (19, 9),\n",
      "    (8, 9),\n",
      "    (1, 10),\n",
      "    (1, 10),\n",
      "    (8, 10),\n",
      "    (0, 10),\n",
      "    (2, 11),\n",
      "    (2, 11),\n",
      "    (4, 11),\n",
      "    (12, 11),\n",
      "    (2, 11),\n",
      "    (17, 11),\n",
      "    (17, 11),\n",
      "    (12, 11),\n",
      "    (2, 11),\n",
      "    (3, 12),\n",
      "    (3, 12),\n",
      "    (2, 12),\n",
      "    (3, 12),\n",
      "    (3, 12),\n",
      "    (3, 12),\n",
      "    (3, 12),\n",
      "    (18, 12),\n",
      "    (3, 12),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (3, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (4, 13),\n",
      "    (2, 14),\n",
      "    (4, 14),\n",
      "    (5, 15),\n",
      "    (6, 15),\n",
      "    (18, 17),\n",
      "    (8, 17),\n",
      "    (1, 17),\n",
      "    (11, 17),\n",
      "    (8, 17),\n",
      "    (19, 18),\n",
      "    (19, 18),\n",
      "    (19, 18),\n",
      "    (0, 18),\n",
      "    (9, 18),\n",
      "    (9, 18),\n",
      "    (16, 18),\n",
      "    (8, 18),\n",
      "    (16, 18),\n",
      "    (0, 18),\n",
      "    (9, 18),\n",
      "    (17, 18),\n",
      "    (5, 19),\n",
      "    (0, 19),\n",
      "    (0, 19),\n",
      "    (18, 19),\n",
      "    (15, 19),\n",
      "    (15, 19),\n",
      "    (14, 19)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mistakes = [\n",
    "    row[0] for row in err_replace\n",
    "]\n",
    "# sort by truth\n",
    "mistakes.sort(key=lambda row:row[1])\n",
    "pp.pprint(\"predicted, truth\")\n",
    "pp.pprint(mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'truth: #, %'\n",
      "{   0: (11, 7.43),\n",
      "    1: (3, 2.03),\n",
      "    2: (7, 4.73),\n",
      "    3: (6, 4.05),\n",
      "    4: (9, 6.08),\n",
      "    5: (6, 4.05),\n",
      "    6: (12, 8.11),\n",
      "    7: (9, 6.08),\n",
      "    8: (12, 8.11),\n",
      "    9: (10, 6.76),\n",
      "    10: (4, 2.7),\n",
      "    11: (9, 6.08),\n",
      "    12: (9, 6.08),\n",
      "    13: (13, 8.78),\n",
      "    14: (2, 1.35),\n",
      "    15: (2, 1.35),\n",
      "    17: (5, 3.38),\n",
      "    18: (12, 8.11),\n",
      "    19: (7, 4.73)}\n",
      "truth:\t#,\t%\tDESC\n",
      "13:\t13\t8.78%\n",
      "6:\t12\t8.11%\n",
      "8:\t12\t8.11%\n",
      "18:\t12\t8.11%\n",
      "0:\t11\t7.43%\n",
      "9:\t10\t6.76%\n",
      "4:\t9\t6.08%\n",
      "7:\t9\t6.08%\n",
      "11:\t9\t6.08%\n",
      "12:\t9\t6.08%\n",
      "2:\t7\t4.73%\n",
      "19:\t7\t4.73%\n",
      "3:\t6\t4.05%\n",
      "5:\t6\t4.05%\n",
      "17:\t5\t3.38%\n",
      "10:\t4\t2.70%\n",
      "1:\t3\t2.03%\n",
      "14:\t2\t1.35%\n",
      "15:\t2\t1.35%\n",
      "ErrPercentage of halfdigits: 42.560000\n"
     ]
    }
   ],
   "source": [
    "# number of occurences of truthvalues\n",
    "truth_occurences = [pair[1] for pair in mistakes]\n",
    "digit, count = np.unique(truth_occurences, return_counts=True)\n",
    "percentages = np.round(100* count / sum(count),2)\n",
    "\n",
    "count_truth = dict(zip(digit, zip(count, percentages)))\n",
    "pp.pprint(\"truth: #, %\")\n",
    "pp.pprint(count_truth)\n",
    "# order by count\n",
    "print(\"truth:\\t#,\\t%\\tDESC\")\n",
    "for key,value in sorted(count_truth.items(), key=lambda item: -item[1][0]):\n",
    "    print(\"%d:\\t%d\\t%.2f%%\"%(key,value[0],value[1]))\n",
    "\n",
    "# relative occurence of all halve-digits\n",
    "#sum_perc = 0\n",
    "#for key,value in count_truth.items():\n",
    "#    if key >= 10:\n",
    "#        sum_perc += value[1]\n",
    "#print(\"ErrPercentage of halfdigits: %2f\"%(sum_perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit:\t#,\t%\tDESC\n",
      "0:\t6612\t33.06%\n",
      "1:\t3234\t16.17%\n",
      "8:\t1538\t7.69%\n",
      "3:\t1214\t6.07%\n",
      "2:\t1100\t5.50%\n",
      "7:\t1037\t5.18%\n",
      "9:\t920\t4.60%\n",
      "5:\t916\t4.58%\n",
      "6:\t754\t3.77%\n",
      "4:\t517\t2.58%\n",
      "13:\t340\t1.70%\n",
      "18:\t245\t1.23%\n",
      "17:\t221\t1.10%\n",
      "19:\t219\t1.10%\n",
      "12:\t215\t1.08%\n",
      "11:\t210\t1.05%\n",
      "16:\t202\t1.01%\n",
      "15:\t184\t0.92%\n",
      "14:\t165\t0.82%\n",
      "10:\t157\t0.78%\n"
     ]
    }
   ],
   "source": [
    "# count digits in traindataset\n",
    "digits, count = np.unique(labels_train.flatten(), return_counts=True)\n",
    "percentages = np.round(100* count / sum(count),2)\n",
    "digit_count = dict(zip(digits, zip(count, percentages)))\n",
    "\n",
    "# order by count\n",
    "print(\"digit:\\t#,\\t%\\tDESC\")\n",
    "for key,value in sorted(digit_count.items(), key=lambda item: -item[1][0]):\n",
    "    print(\"%d:\\t%d\\t%.2f%%\"%(key,value[0],value[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digit\terror%\ttrain%\n",
      "0:\t7.43\t33.06\n",
      "1:\t2.03\t16.17\n",
      "2:\t4.73\t5.50\n",
      "3:\t4.05\t6.07\n",
      "4:\t6.08\t2.58\n",
      "5:\t4.05\t4.58\n",
      "6:\t8.11\t3.77\n",
      "7:\t6.08\t5.18\n",
      "8:\t8.11\t7.69\n",
      "9:\t6.76\t4.60\n",
      "10:\t2.70\t0.78\n",
      "11:\t6.08\t1.05\n",
      "12:\t6.08\t1.08\n",
      "13:\t8.78\t1.70\n",
      "14:\t1.35\t0.82\n",
      "15:\t1.35\t0.92\n",
      "16:\t0.00\t1.01\n",
      "17:\t3.38\t1.10\n",
      "18:\t8.11\t1.23\n",
      "19:\t4.73\t1.10\n"
     ]
    }
   ],
   "source": [
    "# zip digits with number of errors and occurence in traindataset\n",
    "digits = [d for d in range(0,20)]\n",
    "err_perc = [\n",
    "    count_truth.get(digit)[1] if count_truth.get(digit) != None else 0\n",
    "    for digit in digits\n",
    "]\n",
    "perc_in_train = [\n",
    "    digit_count.get(digit)[1]\n",
    "    for digit in digits\n",
    "]\n",
    "\n",
    "print(\"digit\\terror%\\ttrain%\")\n",
    "for digit in digits:\n",
    "    print(\"%d:\\t%.2f\\t%.2f\"%(digit, err_perc[digit], perc_in_train[digit]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errPair \terr#\terr%\n",
      "(4, 13)  \t12\t8.11%\n",
      "(17, 7)  \t8\t5.41%\n",
      "(3, 12)  \t7\t4.73%\n",
      "(19, 9)  \t6\t4.05%\n",
      "(14, 4)  \t5\t3.38%\n",
      "(15, 6)  \t5\t3.38%\n",
      "(10, 0)  \t4\t2.70%\n",
      "(12, 3)  \t4\t2.70%\n",
      "(6, 8)  \t4\t2.70%\n",
      "(18, 8)  \t4\t2.70%\n",
      "(2, 11)  \t4\t2.70%\n",
      "(19, 0)  \t3\t2.03%\n",
      "(13, 4)  \t3\t2.03%\n",
      "(8, 6)  \t3\t2.03%\n",
      "(5, 6)  \t3\t2.03%\n",
      "(19, 18)  \t3\t2.03%\n",
      "(9, 18)  \t3\t2.03%\n",
      "(1, 0)  \t2\t1.35%\n",
      "(11, 2)  \t2\t1.35%\n",
      "(12, 2)  \t2\t1.35%\n",
      "(13, 3)  \t2\t1.35%\n",
      "(15, 5)  \t2\t1.35%\n",
      "(14, 5)  \t2\t1.35%\n",
      "(9, 8)  \t2\t1.35%\n",
      "(18, 9)  \t2\t1.35%\n",
      "(8, 9)  \t2\t1.35%\n",
      "(1, 10)  \t2\t1.35%\n",
      "(12, 11)  \t2\t1.35%\n",
      "(17, 11)  \t2\t1.35%\n",
      "(8, 17)  \t2\t1.35%\n",
      "(0, 18)  \t2\t1.35%\n",
      "(16, 18)  \t2\t1.35%\n",
      "(0, 19)  \t2\t1.35%\n",
      "(15, 19)  \t2\t1.35%\n",
      "(15, 0)  \t1\t0.68%\n",
      "(6, 0)  \t1\t0.68%\n",
      "(7, 1)  \t1\t0.68%\n",
      "(17, 1)  \t1\t0.68%\n",
      "(4, 1)  \t1\t0.68%\n",
      "(14, 2)  \t1\t0.68%\n",
      "(16, 2)  \t1\t0.68%\n",
      "(3, 2)  \t1\t0.68%\n",
      "(11, 4)  \t1\t0.68%\n",
      "(12, 5)  \t1\t0.68%\n",
      "(3, 5)  \t1\t0.68%\n",
      "(16, 6)  \t1\t0.68%\n",
      "(16, 7)  \t1\t0.68%\n",
      "(17, 8)  \t1\t0.68%\n",
      "(3, 8)  \t1\t0.68%\n",
      "(8, 10)  \t1\t0.68%\n",
      "(0, 10)  \t1\t0.68%\n",
      "(4, 11)  \t1\t0.68%\n",
      "(2, 12)  \t1\t0.68%\n",
      "(18, 12)  \t1\t0.68%\n",
      "(3, 13)  \t1\t0.68%\n",
      "(2, 14)  \t1\t0.68%\n",
      "(4, 14)  \t1\t0.68%\n",
      "(5, 15)  \t1\t0.68%\n",
      "(6, 15)  \t1\t0.68%\n",
      "(18, 17)  \t1\t0.68%\n",
      "(1, 17)  \t1\t0.68%\n",
      "(11, 17)  \t1\t0.68%\n",
      "(8, 18)  \t1\t0.68%\n",
      "(17, 18)  \t1\t0.68%\n",
      "(5, 19)  \t1\t0.68%\n",
      "(18, 19)  \t1\t0.68%\n",
      "(14, 19)  \t1\t0.68%\n",
      "total: 148 mistakes\n",
      "ignoring misstate\n",
      "(6, 8)  \t4\t2.70%\n",
      "(8, 6)  \t3\t2.03%\n",
      "(5, 6)  \t3\t2.03%\n",
      "(19, 18)  \t3\t2.03%\n",
      "(1, 0)  \t2\t1.35%\n",
      "(9, 8)  \t2\t1.35%\n",
      "(8, 9)  \t2\t1.35%\n",
      "(12, 11)  \t2\t1.35%\n",
      "(17, 11)  \t2\t1.35%\n",
      "(0, 18)  \t2\t1.35%\n",
      "(16, 18)  \t2\t1.35%\n",
      "(15, 19)  \t2\t1.35%\n",
      "(15, 0)  \t1\t0.68%\n",
      "(6, 0)  \t1\t0.68%\n",
      "(7, 1)  \t1\t0.68%\n",
      "(17, 1)  \t1\t0.68%\n",
      "(4, 1)  \t1\t0.68%\n",
      "(14, 2)  \t1\t0.68%\n",
      "(16, 2)  \t1\t0.68%\n",
      "(3, 2)  \t1\t0.68%\n",
      "(11, 4)  \t1\t0.68%\n",
      "(12, 5)  \t1\t0.68%\n",
      "(3, 5)  \t1\t0.68%\n",
      "(3, 8)  \t1\t0.68%\n",
      "(8, 10)  \t1\t0.68%\n",
      "(4, 11)  \t1\t0.68%\n",
      "(18, 12)  \t1\t0.68%\n",
      "(2, 14)  \t1\t0.68%\n",
      "(18, 17)  \t1\t0.68%\n",
      "(1, 17)  \t1\t0.68%\n",
      "(11, 17)  \t1\t0.68%\n",
      "(17, 18)  \t1\t0.68%\n",
      "(5, 19)  \t1\t0.68%\n",
      "(18, 19)  \t1\t0.68%\n",
      "(14, 19)  \t1\t0.68%\n",
      "total: 52 - 35.14%\n",
      "only misstate\n",
      "(4, 13)  \t12\t8.11%\n",
      "(17, 7)  \t8\t5.41%\n",
      "(3, 12)  \t7\t4.73%\n",
      "(19, 9)  \t6\t4.05%\n",
      "(14, 4)  \t5\t3.38%\n",
      "(15, 6)  \t5\t3.38%\n",
      "(10, 0)  \t4\t2.70%\n",
      "(12, 3)  \t4\t2.70%\n",
      "(18, 8)  \t4\t2.70%\n",
      "(2, 11)  \t4\t2.70%\n",
      "(19, 0)  \t3\t2.03%\n",
      "(13, 4)  \t3\t2.03%\n",
      "(9, 18)  \t3\t2.03%\n",
      "(11, 2)  \t2\t1.35%\n",
      "(12, 2)  \t2\t1.35%\n",
      "(13, 3)  \t2\t1.35%\n",
      "(15, 5)  \t2\t1.35%\n",
      "(14, 5)  \t2\t1.35%\n",
      "(18, 9)  \t2\t1.35%\n",
      "(1, 10)  \t2\t1.35%\n",
      "(8, 17)  \t2\t1.35%\n",
      "(0, 19)  \t2\t1.35%\n",
      "(16, 6)  \t1\t0.68%\n",
      "(16, 7)  \t1\t0.68%\n",
      "(17, 8)  \t1\t0.68%\n",
      "(0, 10)  \t1\t0.68%\n",
      "(2, 12)  \t1\t0.68%\n",
      "(3, 13)  \t1\t0.68%\n",
      "(4, 14)  \t1\t0.68%\n",
      "(5, 15)  \t1\t0.68%\n",
      "(6, 15)  \t1\t0.68%\n",
      "(8, 18)  \t1\t0.68%\n",
      "total: 96 - 64.86%\n",
      "only lower state\n",
      "(6, 8)  \t4\t2.70%\n",
      "(8, 6)  \t3\t2.03%\n",
      "(5, 6)  \t3\t2.03%\n",
      "(1, 0)  \t2\t1.35%\n",
      "(9, 8)  \t2\t1.35%\n",
      "(8, 9)  \t2\t1.35%\n",
      "(6, 0)  \t1\t0.68%\n",
      "(7, 1)  \t1\t0.68%\n",
      "(4, 1)  \t1\t0.68%\n",
      "(3, 2)  \t1\t0.68%\n",
      "(3, 5)  \t1\t0.68%\n",
      "(3, 8)  \t1\t0.68%\n"
     ]
    }
   ],
   "source": [
    "# trainpairs\n",
    "pairCounts = {}\n",
    "sum = 0\n",
    "for pair in mistakes:\n",
    "    if pairCounts.get(pair) ==  None:\n",
    "        pairCounts[pair] = 1\n",
    "        sum += 1\n",
    "    else:\n",
    "        pairCounts[pair] += 1\n",
    "        sum += 1\n",
    "# print all mistakepairs\n",
    "print(\"errPair \\terr#\\terr%\")    \n",
    "for key,value in sorted(pairCounts.items(), key=lambda item: -item[1]):\n",
    "    print(key, \" \\t%d\\t%.2f%%\"%(value,100*value/sum))\n",
    "print(\"total: %d mistakes\"%(sum))\n",
    "\n",
    "# print ignoring misstate\n",
    "print(\"ignoring misstate\")\n",
    "total = 0\n",
    "for key,value in sorted(pairCounts.items(), key=lambda item: -item[1]):\n",
    "    if not test_misstate(key[0], key[1]):\n",
    "        print(key, \" \\t%d\\t%.2f%%\"%(value,100*value/sum))\n",
    "        total += value\n",
    "print(\"total: %d - %.2f%%\"%(total, 100*total/sum))\n",
    "\n",
    "# print only misstate\n",
    "print(\"only misstate\")\n",
    "total = 0\n",
    "for key,value in sorted(pairCounts.items(), key=lambda item: -item[1]):\n",
    "    if test_misstate(key[0], key[1]):\n",
    "        print(key, \" \\t%d\\t%.2f%%\"%(value,100*value/sum))\n",
    "        total += value\n",
    "print(\"total: %d - %.2f%%\"%(total, 100*total/sum))\n",
    "\n",
    "# print only those mistakes where state was mid\n",
    "print(\"only lower state\")\n",
    "for key,value in sorted(pairCounts.items(), key=lambda item: -item[1]):\n",
    "    if key[0] < 10 and key[1] < 10:\n",
    "        print(key, \" \\t%d\\t%.2f%%\"%(value,100*value/sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,10: True\n",
      "0,19: True\n",
      "1,10: True\n",
      "1,11: True\n",
      "2,11: True\n",
      "2,12: True\n",
      "3,12: True\n",
      "3,13: True\n",
      "4,13: True\n",
      "4,14: True\n",
      "5,14: True\n",
      "5,15: True\n",
      "6,15: True\n",
      "6,16: True\n",
      "7,16: True\n",
      "7,17: True\n",
      "8,17: True\n",
      "8,18: True\n",
      "9,18: True\n",
      "9,19: True\n",
      "10,0: True\n",
      "10,1: True\n",
      "11,1: True\n",
      "11,2: True\n",
      "12,2: True\n",
      "12,3: True\n",
      "13,3: True\n",
      "13,4: True\n",
      "14,4: True\n",
      "14,5: True\n",
      "15,5: True\n",
      "15,6: True\n",
      "16,6: True\n",
      "16,7: True\n",
      "17,7: True\n",
      "17,8: True\n",
      "18,8: True\n",
      "18,9: True\n",
      "19,0: True\n",
      "19,9: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_misstate(a,b):\n",
    "    # if both same state, return false\n",
    "    if (a<10 and b<10) or (a>=10 and b>=10):\n",
    "        return False\n",
    "    x = max(a,b);y = min(a,b)\n",
    "    if x == 19 and y == 0:\n",
    "        return True\n",
    "    return (x - y) in [9,10]\n",
    "\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        if test_misstate(i,j):\n",
    "            print(\"%d,%d: %s\"%(i,j,test_misstate(i,j)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
