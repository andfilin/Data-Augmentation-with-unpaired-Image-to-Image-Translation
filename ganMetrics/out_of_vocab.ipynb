{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take an i2i-model where all labels used for training are known.\n",
    "\n",
    "generate a set of translated easy_images using only labels used for training, generate another set using only labels not used in training.\n",
    "\n",
    "calculate FID between these two sets; calculate FID for each set and real easy_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent dir to syspath\n",
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "#root_dir = os.path.dirname(parent_dir)\n",
    "sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from matplotlib import pyplot as plt\n",
    "from imageGenerators import load_realdata\n",
    "import tensorflow as tf\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "from imageGenerators.imgGen_augmented import synth_generator as synth_generator_new\n",
    "from models.cyclegan_modified.cyclegan import cyclegan\n",
    "from FID import FID_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model to test\n",
    "i2i_path = Path(\"D:/m2/savedmodels/goals/q5_lambda/n512_lambda10_2\")\n",
    "i2i_epoch = 2\n",
    "\n",
    "# number of images to generate for each imageset\n",
    "n_images = 1000\n",
    "n_digits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of input to wmn-model\n",
    "fcsrn_height = 48; fcsrn_width = 160; fcsrn_channels = 3\n",
    "fcsrn_shape = (fcsrn_height, fcsrn_width, fcsrn_channels)\n",
    "fcsrn_dims = (fcsrn_width, fcsrn_height)\n",
    "# get shape of inputs for i2i-model\n",
    "inputshapePath = i2i_path / \"inputshape\"\n",
    "i2i_shape = [int(s) for s in inputshapePath.read_text().split(\",\")]\n",
    "i2i_dims = (i2i_shape[1], i2i_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 5)\n"
     ]
    }
   ],
   "source": [
    "# load labels used for training\n",
    "labels_path = i2i_path / \"training_labels.pickle\"\n",
    "with open(labels_path, \"rb\") as picklefile:\n",
    "    labels_synthetic, labels_real = pickle.load(picklefile)\n",
    "# append labels\n",
    "train_labels = np.append(labels_synthetic, labels_real, axis=0)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms label(1d-array of ints) into a string.\n",
    "# used to make a map of labels for quick lookup.\n",
    "def label2string(label):\n",
    "    label = [str(digit)+\",\" for digit in label]\n",
    "    return reduce(lambda state,digit: state+digit, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map with traininglabels as keys \n",
    "labelString_map = [label2string(label) for label in train_labels]\n",
    "labelString_map = dict.fromkeys(labelString_map)\n",
    "\n",
    "# check whether label is in trainset\n",
    "def label_is_in_vocab(label):\n",
    "    s = label2string(label)\n",
    "    return s in labelString_map\n",
    "\n",
    "# assert every label of a labelset is contained in trainset\n",
    "def assert_labelsInVocab(labels):\n",
    "    labels = [label2string(label) for label in labels]\n",
    "    labels = [label in labelString_map for label in labels]\n",
    "    np.testing.assert_array_equal(labels, True)\n",
    "# assert every label of a labelset is not contained in trainset\n",
    "def assert_labelsNotInVocab(labels):\n",
    "    labels = [label2string(label) for label in labels]\n",
    "    labels = [label in labelString_map for label in labels]\n",
    "    np.testing.assert_array_equal(labels, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_in_vocab = train_labels[np.random.randint(0,len(train_labels), (n_images) )]\n",
    "assert_labelsInVocab(labels_in_vocab)\n",
    "\n",
    "labels_notIn_vocab = np.random.randint(0,20, (n_images, n_digits) )\n",
    "for i in range(n_images):\n",
    "    iterations = 0\n",
    "    while label_is_in_vocab(labels_notIn_vocab[i]):\n",
    "        assert iterations < 1000, \"could not find random label not in vocab.\"\n",
    "        labels_notIn_vocab[i] = np.random.randint(0,20, (n_digits) )\n",
    "assert_labelsNotInVocab(labels_notIn_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded checkpoint:  D:\\m2\\savedmodels\\goals\\q5_lambda\\n512_lambda10_2\\epoch-2\n"
     ]
    }
   ],
   "source": [
    "# generate synthetic images\n",
    "synthGenerator = synth_generator_new()\n",
    "synth_inVocab = synthGenerator.makeImages(labels_in_vocab, resizeTo=i2i_dims, color=True, rotate=True)\n",
    "synth_outsideVocab = synthGenerator.makeImages(labels_notIn_vocab, resizeTo=i2i_dims, color=True, rotate=True)\n",
    "\n",
    "# load model, translate synthethic images\n",
    "cgModel = cyclegan(image_shape=i2i_shape,\n",
    "                   n_images=0,\n",
    "                   batchsize=1,\n",
    "                   adversial_loss=\"bce\",\n",
    "                   lr=0,\n",
    "                   _lambda=0,\n",
    "                   poolsize=0,\n",
    "                   checkpoint_path=i2i_path,\n",
    "                   load_checkpoint_after_epoch=i2i_epoch\n",
    "                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_images(synthImages, i2i_model):\n",
    "    model_input = tf.data.Dataset.from_tensor_slices(synthImages)\\\n",
    "                .map(i2i_model.preprocess_input, num_parallel_calls=AUTOTUNE)\\\n",
    "                .cache()\\\n",
    "                .batch(1)\n",
    "    ####\n",
    "    # predict realistic images\n",
    "    translated_images = i2i_model.gen_AtoB.predict(model_input)\n",
    "    # denormalize\n",
    "    translated_images = (translated_images + 1) * 127.5\n",
    "    return translated_images\n",
    "\n",
    "generated_inVocab = translate_images(synth_inVocab, cgModel)\n",
    "generated_outsideVocab = translate_images(synth_outsideVocab, cgModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(image):\n",
    "    plt.imshow(image[:,:,0], cmap=\"gray\", vmin=-1,vmax=1)\n",
    "    plt.show()\n",
    "def showIntImage(image):\n",
    "    plt.imshow(image, cmap=\"gray\", vmin=0,vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\jupyter_ws\\imageGenerators\\load_realdata.py:57: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(txt_path, sep=seperators ,header=None)\n"
     ]
    }
   ],
   "source": [
    "# load real easy samples\n",
    "images_easy, labels_easy = load_realdata.load_wmr_easy(n_toLoad=n_images, resizeTo=i2i_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fid(in,real:\t90)\n",
      "fid(out,real:\t95)\n",
      "fid(in,out:\t13)\n"
     ]
    }
   ],
   "source": [
    "# calculate FID-scores\n",
    "inVocab_stats = FID_interface.calculate_stats(generated_inVocab)\n",
    "outsideVocab_stats = FID_interface.calculate_stats(generated_outsideVocab)\n",
    "real_stats = FID_interface.calculate_stats(images_easy)\n",
    "\n",
    "fid_in_Real = FID_interface.calculate_fid_from_stats(inVocab_stats, real_stats)\n",
    "fid_outside_Real = FID_interface.calculate_fid_from_stats(outsideVocab_stats, real_stats)\n",
    "fid_in_outside = FID_interface.calculate_fid_from_stats(inVocab_stats, outsideVocab_stats)\n",
    "\n",
    "print(\"fid(in,real:\\t%d)\" % (fid_in_Real) ) \n",
    "print(\"fid(out,real:\\t%d)\" % (fid_outside_Real) ) \n",
    "print(\"fid(in,out:\\t%d)\" % (fid_in_outside) ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
